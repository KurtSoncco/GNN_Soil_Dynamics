{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing, radius_graph\n",
    "import enum\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LayerNorm\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch_scatter\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceleration data\n",
    "df = pd.read_csv(\"Data/Data4.csv\")\n",
    "df.drop(columns=[\"0\"], inplace=True)\n",
    "\n",
    "# Coordinates data\n",
    "df_coord = np.array([[2.5, 5*i] for i in range(0, 31)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data_list\n",
    "data_list = []\n",
    "\n",
    "# Time step\n",
    "dt = 1e-4\n",
    "\n",
    "# Establish some data\n",
    "number_trajectories = 1\n",
    "number_ts = -1\n",
    "\n",
    "# Create file_path\n",
    "data_path = \"Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done collecting data!\n",
      "Data saved!\n",
      "Output Location:  Data\\Data4.pt\n"
     ]
    }
   ],
   "source": [
    "for dt in range(df.shape[0]):\n",
    "    if dt == number_ts:\n",
    "        break\n",
    "\n",
    "    # Get acceleration\n",
    "    acceleration = torch.tensor(df.iloc[dt].values, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    # Get edge_index\n",
    "    coordinates = torch.tensor(df_coord, dtype=torch.float)\n",
    "    edges_index = radius_graph(coordinates, r=1.1*5, loop=False).squeeze(0).type(torch.long)\n",
    "\n",
    "    # Get edge_attr\n",
    "    u_i = coordinates[edges_index[0]]\n",
    "    u_j = coordinates[edges_index[1]]\n",
    "    u_ij = u_i - u_j\n",
    "    u_ij_norm = torch.norm(u_ij, p=2, dim=1, keepdim=True)\n",
    "    edge_attr = torch.cat([u_ij, u_ij_norm], dim=-1).type(torch.float)\n",
    "    \n",
    "    # Store data\n",
    "    data_list.append(Data(x=acceleration, edge_index=edges_index, edge_attr=edge_attr, y=acceleration))\n",
    "        \n",
    "print(\"Done collecting data!\")\n",
    "\n",
    "# Save \n",
    "torch.save(data_list, os.path.join(data_path, \"Data4.pt\"))\n",
    "print(\"Data saved!\")\n",
    "print(\"Output Location: \", os.path.join(data_path, \"Data4.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "Normalization is necessary for the features and output parameters to zero mean and unit variance in order to stabilize training. The method defined below, get_stats(), is run before training. It accepts the processed data_list, calculates the mean and standard deviation for the node features, edge features, and node outputs, and normalizes these using the calculated statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(to_normalize,mean_vec,std_vec):\n",
    "    return (to_normalize-mean_vec)/std_vec\n",
    "\n",
    "def unnormalize(to_unnormalize,mean_vec,std_vec):\n",
    "    return to_unnormalize*std_vec+mean_vec\n",
    "\n",
    "def get_stats(data_list):\n",
    "    '''\n",
    "    Method for normalizing processed datasets. Given  the processed data_list,\n",
    "    calculates the mean and standard deviation for the node features, edge features,\n",
    "    and node outputs, and normalizes these using the calculated statistics.\n",
    "    '''\n",
    "\n",
    "    # Mean and std of the node features are calculated\n",
    "    mean_vec_x=torch.zeros(data_list[0].x.shape[1:])\n",
    "    std_vec_x=torch.zeros(data_list[0].x.shape[1:])\n",
    "\n",
    "    # Mean and std of the edge features are calculated\n",
    "    mean_vec_edge=torch.zeros(data_list[0].edge_attr.shape[1:])\n",
    "    std_vec_edge=torch.zeros(data_list[0].edge_attr.shape[1:])\n",
    "\n",
    "    # Mean and std of the output parameters are calculated\n",
    "    mean_vec_y=torch.zeros(data_list[0].y.shape[1:])\n",
    "    std_vec_y=torch.zeros(data_list[0].y.shape[1:])\n",
    "\n",
    "    # Define the maximum number of accumulations to perform such that we do\n",
    "    # not encounter memory issues\n",
    "    max_accumulations = 10**6\n",
    "\n",
    "    #Define a very small value for normalizing to\n",
    "    eps=torch.tensor(1e-8)\n",
    "\n",
    "    #Define counters used in normalization\n",
    "    num_accs_x = 0\n",
    "    num_accs_edge = 0\n",
    "    num_accs_y = 0\n",
    "\n",
    "    #Iterate through the data in the list to accumulate statistics\n",
    "    for dp in data_list:\n",
    "\n",
    "        # Add to the mean and std vectors for the node features\n",
    "        mean_vec_x+=torch.sum(dp.x,dim=0)\n",
    "        std_vec_x+=torch.sum(dp.x**2,dim=0)\n",
    "        num_accs_x+=dp.x.shape[0]\n",
    "\n",
    "        # Add to the mean and std vectors for the edge features\n",
    "        mean_vec_edge+=torch.sum(dp.edge_attr,dim=0)\n",
    "        std_vec_edge+=torch.sum(dp.edge_attr**2,dim=0)\n",
    "        num_accs_edge+=dp.edge_attr.shape[0]\n",
    "\n",
    "        # Add to the mean and std vectors for the node outputs\n",
    "        mean_vec_y+=torch.sum(dp.y,dim=0)\n",
    "        std_vec_y+=torch.sum(dp.y**2,dim=0)\n",
    "        num_accs_y+=dp.y.shape[0]\n",
    "\n",
    "        if(num_accs_x>max_accumulations or num_accs_edge>max_accumulations or num_accs_y>max_accumulations):\n",
    "            break\n",
    "\n",
    "    mean_vec_x = mean_vec_x/num_accs_x\n",
    "    std_vec_x = torch.maximum(torch.sqrt(std_vec_x/num_accs_x - mean_vec_x**2),eps)\n",
    "\n",
    "    mean_vec_edge = mean_vec_edge/num_accs_edge\n",
    "    std_vec_edge = torch.maximum(torch.sqrt(std_vec_edge/num_accs_edge - mean_vec_edge**2),eps)\n",
    "\n",
    "    mean_vec_y = mean_vec_y/num_accs_y\n",
    "    std_vec_y = torch.maximum(torch.sqrt(std_vec_y/num_accs_y - mean_vec_y**2),eps)\n",
    "\n",
    "    mean_std_list=[mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y]\n",
    "\n",
    "    return mean_std_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim_node, input_dim_edge, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.node_encoder = Sequential(Linear(input_dim_node , hidden_dim),\n",
    "                                ReLU(),\n",
    "                                Linear( hidden_dim, hidden_dim),\n",
    "                                LayerNorm(hidden_dim))\n",
    "        \n",
    "        self.edge_encoder = Sequential(Linear(input_dim_edge , hidden_dim),\n",
    "                                ReLU(),\n",
    "                                Linear( hidden_dim, hidden_dim),\n",
    "                                LayerNorm(hidden_dim))\n",
    "\n",
    "    def forward(self, x, edge_attr):\n",
    "        x = self.node_encoder(x)\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "        return x, edge_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim_node):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.node_decoder = Sequential(Linear(hidden_dim, hidden_dim),\n",
    "                                ReLU(),\n",
    "                                Linear(hidden_dim, output_dim_node))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.node_decoder(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshGraphNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim_node, input_dim_edge, hidden_dim, output_dim, args, emb=False):\n",
    "        super(MeshGraphNet, self).__init__()\n",
    "\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(input_dim_node, input_dim_edge, hidden_dim)\n",
    "\n",
    "        # Processor\n",
    "        self.processor = nn.ModuleList()\n",
    "        assert (self.num_layers >= 1), 'Number of message passing layers is not >=1'\n",
    "        \n",
    "        processor_layer=self.build_processor_model()\n",
    "        for _ in range(self.num_layers):\n",
    "            self.processor.append(processor_layer(hidden_dim,hidden_dim))\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(hidden_dim, output_dim)\n",
    "\n",
    "    def build_processor_model(self):\n",
    "        return ProcessorLayer\n",
    "\n",
    "    def forward(self,data,mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge):\n",
    "\n",
    "        x, edge_attr, edge_index = data.x, data.edge_attr, data.edge_index\n",
    "\n",
    "        # Normalize the input data\n",
    "        x = normalize(x,mean_vec_x,std_vec_x)\n",
    "        edge_attr = normalize(edge_attr,mean_vec_edge,std_vec_edge)\n",
    "\n",
    "        # Encoder\n",
    "        x, edge_attr = self.encoder(x, edge_attr)\n",
    "\n",
    "        # Processor\n",
    "        for i in range(self.num_layers):\n",
    "            x, edge_attr = self.processor[i](x, edge_index, edge_attr)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, pred, inputs, mean_vec_y, std_vec_y):\n",
    "\n",
    "        # Normalize the output data\n",
    "        y = inputs.y\n",
    "        y = normalize(y,mean_vec_y,std_vec_y)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = torch.sum((pred-y)**2, axis=1)\n",
    "\n",
    "        # Calculate the sqrt loss\n",
    "        loss = torch.sqrt(loss)\n",
    "\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProcessorLayer Class: Edge Message Passing, Aggregation, and Updating\n",
    "\n",
    "Now let's implement the processor, which overrides \"[MessagePassing](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html)\" base class. Following the prototype of the base class, we need to implement three main methods, namely message passing, aggregation, and updating. Also, two types of MLP layers, namely node MLP and edge MLP, are defined and used during the construction of processor, whose details will be given in the cell bellow.\n",
    "\n",
    "Essentailly, our processor class serves as the GNN layers composed of message passing, aggregation, and updating, updating information at each layer of the computational graph for each node. The message passing process can be described as:\n",
    "\n",
    "1.   **Message passing**\n",
    "\n",
    "Initiated by the propagate function, the message function most generally calculates messages, m, for edge u at layer l with function MSG given previous embeddings h_u:\n",
    "$$m_u^{(l)}=MSG^{(l)}(h_u^{(l-1)})$$\n",
    "\n",
    "Note that for MeshGraphNets, messages are calculated for edges and passed to nodes. This function thus takes edge embeddings and the adjacent node embeddings and concatenates them. These concatenated previous embeddings constitute h_u above. These are then put through an MLP (our MSG function) to give the final messages, m_u, which are passed to the aggregate function.\n",
    "\n",
    "2.   **Aggregation**\n",
    "\n",
    "Aggregation takes the updated edge embeddings and aggregates then over the connectivity matrix indexing using sum reduction. Most generally, we have:\n",
    "\n",
    "$$h_v^{(l)}=AGG^{(l)}(\\{m_u^{(l)},u\\in N(v)\\})$$\n",
    "\n",
    "For MeshGraphNets, aggregation (AGG) for node v is sum over the neighbor nodes. However, there is also an additional aggregation step: aggregating with the self embedding. This is done outside of the aggregation function, in the forward function after the return of propagate:\n",
    "\n",
    "$$h_v^{(l)}=\\{h_v^{(l-1)},AGG^{(l)}(\\{m_u^{(l)},u\\in N(v)\\})\\}$$\n",
    "\n",
    "3.   **Updating**\n",
    "\n",
    "The nodes embeddings are finally updated by passing $h_v^{(l)}$ through the node MLP with a skip connection. This is most generally written as:\n",
    "\n",
    "$$h_v^{(l)}=Processor(h_v^{(l)})$$\n",
    "\n",
    "Where for us the Processor is an MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessorLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(ProcessorLayer, self).__init__(**kwargs)\n",
    "\n",
    "        # Note that the node and edge encoders both have the same hidden dimension\n",
    "        # size. This means that the input of the edge processor will always be\n",
    "        # three times the specified hidden dimension\n",
    "        # (input: adjacent node embeddings and self embeddings)\n",
    "        self.edge_mlp = Sequential(Linear( 3* in_channels , out_channels),\n",
    "                                   ReLU(),\n",
    "                                   Linear( out_channels, out_channels),\n",
    "                                   LayerNorm(out_channels))\n",
    "\n",
    "        self.node_mlp = Sequential(Linear( 2* in_channels , out_channels),\n",
    "                                   ReLU(),\n",
    "                                   Linear( out_channels, out_channels),\n",
    "                                   LayerNorm(out_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.edge_mlp[0].reset_parameters()\n",
    "        self.edge_mlp[2].reset_parameters()\n",
    "\n",
    "        self.node_mlp[0].reset_parameters()\n",
    "        self.node_mlp[2].reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, size=None):\n",
    "\n",
    "        out, updated_edges = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size) # Out has the shape of [E, out_channels]\n",
    "        updated_nodes = torch.cat([x, out], dim=1) # Complete the aggregation through self-aggregation\n",
    "        updated_nodes = x + self.node_mlp(updated_nodes) # Residual connection\n",
    "\n",
    "        return updated_nodes, updated_edges\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "\n",
    "        updated_edges = torch.cat([x_i, x_j, edge_attr], dim=1) # Shape of [E, 3*in_channels]\n",
    "        updated_edges = self.edge_mlp(updated_edges)\n",
    "\n",
    "        return updated_edges\n",
    "\n",
    "    def aggregate(self, updated_edges, edge_index, dim_size = None):\n",
    "        \n",
    "        # The axis along which to index number of nodes.\n",
    "        node_dim = 0\n",
    "\n",
    "        out = torch_scatter.scatter(updated_edges, edge_index[0, :], dim=node_dim, reduce = 'sum')\n",
    "\n",
    "        return out, updated_edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"num_layers\": 2,\n",
    "    \"hidden_dim\": 64,\n",
    "    \"output_dim\": 1\n",
    "}\n",
    "\n",
    "args = objectview(args)\n",
    "model = MeshGraphNet(1, 3, 64, 1, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model works!\n"
     ]
    }
   ],
   "source": [
    "stats = get_stats(data_list)\n",
    "mean_vec_x, std_vec_x, mean_vec_edge, std_vec_edge, mean_vec_y, std_vec_y = stats\n",
    "\n",
    "model(data_list[0], mean_vec_x, std_vec_x, mean_vec_edge, std_vec_edge)\n",
    "print(\"Model works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, stats_list, args, device=\"cpu\"):\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"Epoch\", \"Loss\"])\n",
    "\n",
    "    # Define the model name\n",
    "    model_name = 'model_nl'+str(args.num_layers)+'_bs'+str(args.batch_size) + \\\n",
    "               '_hd'+str(args.hidden_dim)+'_ep'+str(args.epochs)+'_wd'+str(args.weight_decay) + \\\n",
    "               '_lr'+str(args.lr)+'_shuff_'+str(args.shuffle)+'_tr'+str(args.train_size)+'_te'+str(args.test_size)\n",
    "\n",
    "    # DataLoader\n",
    "    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # The statistics of the data decomposed\n",
    "    [mean_vec_x, std_vec_x, mean_vec_edge, std_vec_edge, mean_vec_y, std_vec_y] = stats_list\n",
    "    (mean_vec_x, std_vec_x, mean_vec_edge, std_vec_edge, mean_vec_y, std_vec_y) = (mean_vec_x.to(device), std_vec_x.to(device), mean_vec_edge.to(device), std_vec_edge.to(device), mean_vec_y.to(device), std_vec_y.to(device))\n",
    "\n",
    "    # Build model\n",
    "    num_node_features = dataset[0].x.shape[1]\n",
    "    num_edge_features = dataset[0].edge_attr.shape[1]\n",
    "    output_dim = 1\n",
    "\n",
    "    model = MeshGraphNet(num_node_features, num_edge_features, args.hidden_dim, output_dim, args).to(device)\n",
    "    scheduler, optimizer = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # Training\n",
    "    losses = []\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        num_loops = 0\n",
    "        for batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch, mean_vec_x, std_vec_x, mean_vec_edge, std_vec_edge)\n",
    "            loss = model.loss(pred, batch, mean_vec_y, std_vec_y)\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            num_loops += 1\n",
    "\n",
    "        total_loss /= num_loops\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Loss: {total_loss}\")\n",
    "\n",
    "    return model, losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "for args in [\n",
    "        {'model_type': 'meshgraphnet',\n",
    "         'num_layers': 2,\n",
    "         'batch_size': 16,\n",
    "         'hidden_dim': 10,\n",
    "         'epochs': 500,\n",
    "         'opt': 'adam',\n",
    "         'opt_scheduler': 'none',\n",
    "         'opt_restart': 0,\n",
    "         'weight_decay': 5e-4,\n",
    "         'lr': 0.001,\n",
    "         'train_size': 45,\n",
    "         'test_size': 10,\n",
    "         'device':'cpu',\n",
    "         'shuffle': True,\n",
    "         'save_velo_val': True,\n",
    "         'save_best_model': True,\n",
    "         'checkpoint_dir': './best_models/',\n",
    "         'postprocess_dir': './2d_loss_plots/'},\n",
    "    ]:\n",
    "        args = objectview(args)\n",
    "\n",
    "torch.manual_seed(5)  #Torch\n",
    "random.seed(5)        #Python\n",
    "np.random.seed(5)     #NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = torch.load(\"Data/Data4.pt\")\n",
    "\n",
    "stats_list = get_stats(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/500 [00:00<?, ?Epochs/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 5/500 [01:19<2:10:57, 15.87s/Epochs]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[107], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset, stats_list, args, device)\u001b[0m\n\u001b[0;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     32\u001b[0m num_loops \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 33\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kurt-\\miniconda3\\envs\\ML\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\kurt-\\miniconda3\\envs\\ML\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\kurt-\\miniconda3\\envs\\ML\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch_geometric\\loader\\dataloader.py:27\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     25\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch_geometric\\data\\batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch_geometric\\data\\collate.py:143\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (add_batch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stores[\u001b[38;5;241m0\u001b[39m], NodeStorage)\n\u001b[0;32m    141\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m stores[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcan_infer_num_nodes):\n\u001b[0;32m    142\u001b[0m         repeats \u001b[38;5;241m=\u001b[39m [store\u001b[38;5;241m.\u001b[39mnum_nodes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m stores]\n\u001b[1;32m--> 143\u001b[0m         out_store\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m \u001b[43mrepeat_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m         out_store\u001b[38;5;241m.\u001b[39mptr \u001b[38;5;241m=\u001b[39m cumsum(torch\u001b[38;5;241m.\u001b[39mtensor(repeats, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, slice_dict, inc_dict\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch_geometric\\data\\collate.py:315\u001b[0m, in \u001b[0;36mrepeat_interleave\u001b[1;34m(repeats, device)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepeat_interleave\u001b[39m(\n\u001b[0;32m    312\u001b[0m     repeats: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    313\u001b[0m     device: Optional[torch\u001b[38;5;241m.\u001b[39mdevice] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 315\u001b[0m     outs \u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(repeats)]\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(outs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model, losses = train(dataset, stats_list, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
